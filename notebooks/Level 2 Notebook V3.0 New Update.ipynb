{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d47b83-3a1b-46c3-b23b-5f142dbec9e6",
   "metadata": {},
   "source": [
    "# Level 2: Rice Crop Yield Forecasting Tool Benchmark Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9d7d53-a5f2-4db9-b352-13f60f836489",
   "metadata": {},
   "source": [
    "## Challenge Level 2 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7167ea-5f1c-48c6-ad88-e34e9bb6b4c4",
   "metadata": {},
   "source": [
    "<p align=\"justify\">Welcome to the EY Open Science Data Challenge 2023! This challenge consists of two levels – Level 1 and Level 2. This is the Level 2 challenge aimed at participants who have intermediate or advanced skill sets in data science and programming. The goal of Level 2 is to predict the yield of rice crop at a given location using satellite data. By the time you complete this level, you would have developed a rice crop yield forecasting model, which can predict the yield of rice crop.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bee540-db29-4fa2-999c-09a4a5b2975d",
   "metadata": {},
   "source": [
    "<b>Challenge Aim: </b><p align=\"justify\"> <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098963b6-4e9f-463f-8968-ee0b266eec58",
   "metadata": {},
   "source": [
    "<p align=\"justify\">In this notebook, we will demonstrate a basic model workflow that can serve as a starting point for the challenge. The basic model has been built to predict the yield of  rice crop in Vietnam using features from Sentinel-1 Radiometrically Terrain Corrected (RTC)  dataset as predictor variables. In this demonstration, we have used statistical features generated from the bands (VV and VH) of the Sentinel-1 RTC dataset and mathematical combinations of these bands (VV/VH). We have trained an extra tree regressor model with these features. We have extracted the VV and VH band data from the Sentinel-1 dataset for summer autumn (SA) /winter spring (WS) season for the year 2022 based on the data provided.\n",
    "\n",
    "Most of the functions presented in this notebook were adapted from the <a href=\"https://planetarycomputer.microsoft.com/dataset/sentinel-1-rtc#Example-Notebook\">Sentinel-1-RTC notebook</a> found in the Planetary Computer portal.</p>\n",
    "    \n",
    "<p align=\"justify\"> Please note that this notebook is just a starting point. We have made many assumptions in this notebook that you may think are not best for solving the challenge effectively. You are encouraged to modify these functions, rewrite them, or try an entirely new approach.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f476c1ba-a6e4-477b-be3d-dcb9135a18f2",
   "metadata": {},
   "source": [
    "## Load In Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bb0e63-fc52-459c-a385-2d62e817fa19",
   "metadata": {},
   "source": [
    "To run this demonstration notebook, you will need to have the following packages imported below installed. This may take some time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aae583-3c45-4d46-9fd7-7bb4629f38ba",
   "metadata": {},
   "source": [
    "#### Note: Environment setup\n",
    "Running this notebook requires an API key.\n",
    "\n",
    "Please use <b>planetary_computer.settings.set_subscription_key</b> (<i style=\"color:#eb2f2f;\">API Key</i>) and pass your API key here.\n",
    "\n",
    "See <a href=\"https://planetarycomputer.microsoft.com/docs/concepts/sas/#when-an-account-is-needed\">when an account is needed for more </a>, and <a href=\"https://planetarycomputer.microsoft.com/account/request\">request</a> an account if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eba2110-c3ad-4136-849e-14ffa70decd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import ipyleaflet\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# Planetary Computer Tools\n",
    "import pystac\n",
    "import pystac_client\n",
    "import odc\n",
    "from pystac_client import Client\n",
    "from pystac.extensions.eo import EOExtension as eo\n",
    "from odc.stac import stac_load\n",
    "import planetary_computer as pc\n",
    "\n",
    "# Please pass your API key here\n",
    "pc.settings.set_subscription_key('6d4762f1152d42a285532dd26ea62836')\n",
    "\n",
    "# Others\n",
    "import requests\n",
    "import rich.table\n",
    "from itertools import cycle\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9323515-02ec-4a3b-b64e-14c368c6908c",
   "metadata": {},
   "source": [
    "## Response Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0829a-91d4-4a1d-af55-ab359a1536e0",
   "metadata": {},
   "source": [
    "Before building the model, we need to load in the rice crop yield data. In particular, rice crop yield data was collected for the period of late-2021 to mid-2022 over the Chau Phu, Chau Thanh and Thoai Son districts.\n",
    "\n",
    "This is a dense rice crop region with a mix of double and triple cropping cycles.For this demonstration, we have assumed a triple cropping (3 cycles per year) for all the data points, but you are free to explore the impact of cropping cycles on the yield.You will have to map every data point with its corresponding crop cycle.\n",
    "The crop cycles are Winter-Spring ( November – April) and the Summer-Autumn (April – August). E.g., the harvest date for the first entry is 15th July 2022. The corresponding crop cycle will be Summer-Autumn (April – August). \n",
    "\n",
    "The data consists of geo locations (Latitude and Longitude), District, Season, Rice Crop Intensity, Date of Harvest, Field Size (in Hectares) with the yield in each geo location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbc540b6-319d-4ba4-b699-e96264de6855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>District</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Season(SA = Summer Autumn, WS = Winter Spring)</th>\n",
       "      <th>Rice Crop Intensity(D=Double, T=Triple)</th>\n",
       "      <th>Date of Harvest</th>\n",
       "      <th>Field size (ha)</th>\n",
       "      <th>Rice Yield (kg/ha)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chau_Phu</td>\n",
       "      <td>10.510542</td>\n",
       "      <td>105.248554</td>\n",
       "      <td>SA</td>\n",
       "      <td>T</td>\n",
       "      <td>15-07-2022</td>\n",
       "      <td>3.40</td>\n",
       "      <td>5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chau_Phu</td>\n",
       "      <td>10.509150</td>\n",
       "      <td>105.265098</td>\n",
       "      <td>SA</td>\n",
       "      <td>T</td>\n",
       "      <td>15-07-2022</td>\n",
       "      <td>2.43</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chau_Phu</td>\n",
       "      <td>10.467721</td>\n",
       "      <td>105.192464</td>\n",
       "      <td>SA</td>\n",
       "      <td>D</td>\n",
       "      <td>15-07-2022</td>\n",
       "      <td>1.95</td>\n",
       "      <td>6400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chau_Phu</td>\n",
       "      <td>10.494453</td>\n",
       "      <td>105.241281</td>\n",
       "      <td>SA</td>\n",
       "      <td>T</td>\n",
       "      <td>15-07-2022</td>\n",
       "      <td>4.30</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chau_Phu</td>\n",
       "      <td>10.535058</td>\n",
       "      <td>105.252744</td>\n",
       "      <td>SA</td>\n",
       "      <td>D</td>\n",
       "      <td>14-07-2022</td>\n",
       "      <td>3.30</td>\n",
       "      <td>6400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   District   Latitude   Longitude  \\\n",
       "0  Chau_Phu  10.510542  105.248554   \n",
       "1  Chau_Phu  10.509150  105.265098   \n",
       "2  Chau_Phu  10.467721  105.192464   \n",
       "3  Chau_Phu  10.494453  105.241281   \n",
       "4  Chau_Phu  10.535058  105.252744   \n",
       "\n",
       "  Season(SA = Summer Autumn, WS = Winter Spring)  \\\n",
       "0                                             SA   \n",
       "1                                             SA   \n",
       "2                                             SA   \n",
       "3                                             SA   \n",
       "4                                             SA   \n",
       "\n",
       "  Rice Crop Intensity(D=Double, T=Triple) Date of Harvest  Field size (ha)  \\\n",
       "0                                       T      15-07-2022             3.40   \n",
       "1                                       T      15-07-2022             2.43   \n",
       "2                                       D      15-07-2022             1.95   \n",
       "3                                       T      15-07-2022             4.30   \n",
       "4                                       D      14-07-2022             3.30   \n",
       "\n",
       "   Rice Yield (kg/ha)  \n",
       "0                5500  \n",
       "1                6000  \n",
       "2                6400  \n",
       "3                6000  \n",
       "4                6400  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_yield_data = pd.read_csv(\"../data/train.csv\")\n",
    "crop_yield_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccc0c2e-e01e-4619-a8fa-50090cdb18f0",
   "metadata": {},
   "source": [
    "## Predictor Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1874a6dd-ffa2-4fb6-baa2-49d8dc7a92c2",
   "metadata": {},
   "source": [
    "<p align =\"justify\">Now that we have our crop yield data, it is time to gather and generate the predictor variables from the Sentinel-1 dataset. For a more in-depth look regarding the Sentinel-1 dataset and how to query it, see the Sentinel-1 <a href=\"https://challenge.ey.com/api/v1/storage/admin-files/6403146221623637-63ca8d537b1fe300146c79d0-Sentinel%201%20Phenology.ipynb/\"> supplementary \n",
    "notebook</a>.\n",
    "\n",
    "   \n",
    "\n",
    "<p align = \"justify\">Sentinel-1 radar data penetrates through the clouds, thus helping us to get the band values with minimal atmospheric attenuation. Here we are generating timeseries band values over a period of four months.</p>\n",
    "\n",
    "<p align = \"justify\">\n",
    "A time series data is made up of data points that are collected at regular intervals and are dependent on one another. Many of the tasks involved in data modelling depend heavily on feature engineering. This is only a technique that identifies key aspects of the data that a model might use to improve performance. Because time series modelling uses sequential data that is produced by changes in any value over time, feature engineering operates differently in this context. Creation of statistical features using time series data is one of the feature engineering techniques. Here, we create statistical features using the band values (VV and VH) and the mathematical combination of band values (VV/VH) from Sentinel-1 dataset that aid in predicting the rice yield.\n",
    "</p>\n",
    "<ul>\n",
    "<li>VV - gamma naught values of signal transmitted with vertical polarization and received with vertical polarization with radiometric terrain correction applied.\n",
    "\n",
    "<li>VH - gamma naught values of signal transmitted with vertical polarization and received with horizontal polarization with radiometric terrain correction applied.\n",
    "       \n",
    "</ul>\n",
    "\n",
    "    \n",
    "<p align = \"justify\"><b> Note : Any model utilizing “season” as predictor will be ruled invalid. Examples of seasons include Winter Spring, Summer Autumn etc. But you can use season information to extract the satellite data.</b></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3595690b-c081-4830-bd4f-75c1cdfd9db8",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(195, 52, 235)\"><strong>Tip 1</strong></h4>\n",
    "<p align=\"justify\">Participants can consider the use of optical data from Sentinel-2 and Landsat. All of these datasets are readily available from the <a href=\"https://planetarycomputer.microsoft.com/\"> Microsoft Planetary Computer</a>. Participants can choose one or more of these satellite datasets for their solution. Sentinel-1 radar data penetrates through the clouds, thus helping us to get the band values with minimal atmospheric attenuation, whereas the data from the Sentinel-2 and Landsat data may contain attenuation due to the presence of cloud.</p>\n",
    "\n",
    "<p align=\"justify\"> Participants should also note that Sentinel-1 provides a consistent 12 day revist whereas the optical data may be missing due to extreme cloud cover for an entire scene or particular pixels having cloud contanimation. Please refer the sample notebooks provided for <a href=\"https://challenge.ey.com/api/v1/storage/admin-files/6403146221623637-63ca8d537b1fe300146c79d0-Sentinel%201%20Phenology.ipynb\">Sentinel-1</a>, <a href=\"https://challenge.ey.com/api/v1/storage/admin-files/200864767105553-63ca8c57aea56e00146e319c-Sentinel%202%20cloud%20filtering.ipynb\">Sentinel-2</a> and <a href=\"https://challenge.ey.com/api/v1/storage/admin-files/36808312288709755-63ca8ccb7b1fe300146c7917-Landsat%20cloud%20filtering.ipynb\">Landsat</a> to get more details about filtering and using these datasets.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c183b18-7998-47ff-9bd4-c4d655b52eb0",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(195, 52, 235)\"><strong>Tip 2</strong></h4>\n",
    "<p align=\"justify\">Participants might explore other combinations of bands from the Sentinel-1 data or from other satellites. For example, you can use mathematical combinations of bands to generate various <a href=\"https://challenge.ey.com/api/v1/storage/admin-files/3868217534768359-63ca8dc8aea56e00146e3489-Comprehensive%20Guide%20-%20Satellite%20Data.docx\">vegetation indices </a> which can then be used as features in your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a4af24-5841-436d-ae3b-c3fd7c0bea45",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(195, 52, 235)\"><strong>Tip 3</strong></h4>\n",
    "<p align =\"justify\"> Participants are suggested to choose the time of interest based on the phenology curves and comprehend the patterns of the rice cycle rather than just choosing the first and last day of the season.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2fc63b-34b0-429d-8026-6297e2fc2728",
   "metadata": {},
   "source": [
    "### Accessing the Sentinel-1 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ef3c7-5738-45a6-acd0-dc3083b27eda",
   "metadata": {},
   "source": [
    "<p align = \"Justify\">To get the Sentinel-1 data, we write a function called <i><b>get_sentinel_data.</b></i> This function will fetch VV, VH band values and VV/VH values for a particular location over the specified time window. In this example, we have taken the VV, VH, and VV/VH values for 4 months in each season.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e0d09d7-49f6-4e1c-9fd4-873a577a33c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentinel_data(longitude, latitude, season,assests):\n",
    "    \n",
    "    '''\n",
    "    Returns a list of VV,VH, VV/VH values for a given latitude and longitude over a given time period (based on the season)\n",
    "    Attributes:\n",
    "    longitude - Longitude\n",
    "    latitude - Latitude\n",
    "    season - The season for which band values need to be extracted.\n",
    "    assets - A list of bands to be extracted\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    bands_of_interest = assests\n",
    "    if season == 'SA':\n",
    "        time_slice = \"2022-05-01/2022-08-31\"\n",
    "    if season == 'WS':\n",
    "        time_slice = \"2022-01-01/2022-04-30\"\n",
    "        \n",
    "    vv_list = []\n",
    "    vh_list = []\n",
    "    vv_by_vh_list = []\n",
    "    \n",
    "    bbox_of_interest = [longitude , latitude, longitude, latitude]\n",
    "    time_of_interest = time_slice\n",
    "    \n",
    "    catalog = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "    search = catalog.search(collections=[\"sentinel-1-rtc\"], bbox=bbox_of_interest, datetime=time_of_interest)\n",
    "    items = list(search.get_all_items())\n",
    "    item = items[0]\n",
    "    items.reverse()\n",
    "    \n",
    "    data = stac_load([items[1]],bands=bands_of_interest, patch_url=pc.sign, bbox=bbox_of_interest).isel(time=0)\n",
    "\n",
    "    for item in items:\n",
    "        data = stac_load([item], bands=bands_of_interest, patch_url=pc.sign, bbox=bbox_of_interest).isel(time=0)\n",
    "        if(data['vh'].values[0][0]!=-32768.0 and data['vv'].values[0][0]!=-32768.0):\n",
    "            data = data.where(~data.isnull(), 0)\n",
    "            vh = data[\"vh\"].astype(\"float64\")\n",
    "            vv = data[\"vv\"].astype(\"float64\")\n",
    "            vv_list.append(np.median(vv))\n",
    "            vh_list.append(np.median(vh))\n",
    "            vv_by_vh_list.append(np.median(vv)/np.median(vh))\n",
    "              \n",
    "    return vv_list, vh_list, vv_by_vh_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b445abb2-735c-4fa7-b62d-c31e974e3027",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(195, 52, 235)\"><strong>Tip 4 </strong></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a314a629-a686-4f81-b191-483c4111bd63",
   "metadata": {},
   "source": [
    "Explore the approach of building a bounding box (e.g., 5x5 pixels) around the given latitude and longitude positions and then extract the aggregated band values (e.g., mean, median) to get normalized band values to build the model. Radar data has inherent variability at the pixel level due to variable scattering response from the target. This effect is called “speckle” and it is common to filter the data to smooth these variations. Try using a 3x3, 5x5 or 7x7 window around the specific latitude and longitude point to get improved results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a827365a-01ed-41f8-9973-6480e4402b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab204e84fc974b9994e84670acc49444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/557 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Get Sentinel-1-RTC Data\n",
    "assests = ['vh','vv']\n",
    "train_band_values=crop_yield_data.progress_apply(lambda x: get_sentinel_data(x['Longitude'], x['Latitude'],x['Season(SA = Summer Autumn, WS = Winter Spring)'],assests), axis=1)\n",
    "vh = [x[0] for x in train_band_values]\n",
    "vv = [x[1] for x in train_band_values]\n",
    "vv_by_vh = [x[2] for x in train_band_values]\n",
    "vh_vv_data = pd.DataFrame(list(zip(vh,vv,vv_by_vh)),columns = [\"vv_list\",\"vh_list\",\"vv/vh_list\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac658fb3-599f-42ab-ab90-62881b71a6b1",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Feature engineering, in simple terms, is the act of converting raw observations into desired features using statistical or machine learning approaches. Feature engineering refers to the process of designing artificial features into an algorithm. These artificial features are then used by that algorithm in order to improve its performance, or in other words reap better results. \n",
    "#### Creating some statistical features from the band values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b8fe8-9d0f-4d4a-b93a-95ec05ccd45d",
   "metadata": {},
   "source": [
    "Now let us generate few statistical features. Here we generate 6 features for VV, VH and VV/VH. The six statistical features are:\n",
    "<ul>\n",
    "    <li>Minimum</li>\n",
    "    <li>Maximum</li>\n",
    "    <li>Range</li>\n",
    "    <li>Mean</li> \n",
    "    <li>Auto Correlation</li>\n",
    "    <li>Permutation Entropy</li>\n",
    "</ul>\n",
    "\n",
    "<p align=\"justify\">\n",
    "Auto Correlation - Autocorrelation represents the degree of similarity between a given time series and a lagged version of itself over successive time intervals. Autocorrelation measures the relationship between a variable's current value and its past values.\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "Permutation Entropy - Permutation Entropy (PE) is a robust time series tool which provides a quantification measure of the complexity of a dynamic system by capturing the order relations between values of a time series and extracting a probability distribution of the ordinal patterns.\n",
    "</p>\n",
    "<p>You are encouraged to identify possible time series metrices that can be used as features.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d0a846-c527-4cc4-a277-03ffa558f5f9",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(195, 52, 235)\"><strong>Tip 5 </strong></h4>\n",
    "Participants can generate other statistical features which are statiscally significant to understand characterstics of rice phenology. There are existing packages available which can generate some of these metrics for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f8fae7-2601-4fa1-8406-0ad6791a1c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinal_distribution(data, dx=3, dy=1, taux=1, tauy=1, return_missing=False, tie_precision=None):\n",
    "    '''\n",
    "    Returns\n",
    "    -------\n",
    "     : tuple\n",
    "       Tuple containing two arrays, one with the ordinal patterns occurring in data \n",
    "       and another with their corresponding probabilities.\n",
    "       \n",
    "    Attributes\n",
    "    ---------\n",
    "    data : array \n",
    "           Array object in the format :math:`[x_{1}, x_{2}, x_{3}, \\\\ldots ,x_{n}]`\n",
    "           or  :math:`[[x_{11}, x_{12}, x_{13}, \\\\ldots, x_{1m}],\n",
    "           \\\\ldots, [x_{n1}, x_{n2}, x_{n3}, \\\\ldots, x_{nm}]]`.\n",
    "    dx : int\n",
    "         Embedding dimension (horizontal axis) (default: 3).\n",
    "    dy : int\n",
    "         Embedding dimension (vertical axis); it must be 1 for time series \n",
    "         (default: 1).\n",
    "    taux : int\n",
    "           Embedding delay (horizontal axis) (default: 1).\n",
    "    tauy : int\n",
    "           Embedding delay (vertical axis) (default: 1).\n",
    "    return_missing: boolean\n",
    "                    If `True`, it returns ordinal patterns not appearing in the \n",
    "                    symbolic sequence obtained from **data** are shown. If `False`,\n",
    "                    these missing patterns (permutations) are omitted \n",
    "                    (default: `False`).\n",
    "    tie_precision : int\n",
    "                    If not `None`, **data** is rounded with `tie_precision`\n",
    "                    number of decimals (default: `None`).\n",
    "   \n",
    "    '''\n",
    "    def setdiff(a, b):\n",
    "        '''\n",
    "        Returns\n",
    "        -------\n",
    "        : array\n",
    "            An array containing the elements in `a` that are not contained in `b`.\n",
    "            \n",
    "        Parameters\n",
    "        ----------    \n",
    "        a : tuples, lists or arrays\n",
    "            Array in the format :math:`[[x_{21}, x_{22}, x_{23}, \\\\ldots, x_{2m}], \n",
    "            \\\\ldots, [x_{n1}, x_{n2}, x_{n3}, ..., x_{nm}]]`.\n",
    "        b : tuples, lists or arrays\n",
    "            Array in the format :math:`[[x_{21}, x_{22}, x_{23}, \\\\ldots, x_{2m}], \n",
    "            \\\\ldots, [x_{n1}, x_{n2}, x_{n3}, ..., x_{nm}]]`.\n",
    "        '''\n",
    "\n",
    "        a = np.asarray(a).astype('int64')\n",
    "        b = np.asarray(b).astype('int64')\n",
    "\n",
    "        _, ncols = a.shape\n",
    "\n",
    "        dtype={'names':['f{}'.format(i) for i in range(ncols)],\n",
    "            'formats':ncols * [a.dtype]}\n",
    "\n",
    "        C = np.setdiff1d(a.view(dtype), b.view(dtype))\n",
    "        C = C.view(a.dtype).reshape(-1, ncols)\n",
    "\n",
    "        return(C)\n",
    "\n",
    "    try:\n",
    "        ny, nx = np.shape(data)\n",
    "        data   = np.array(data)\n",
    "    except:\n",
    "        nx     = np.shape(data)[0]\n",
    "        ny     = 1\n",
    "        data   = np.array([data])\n",
    "\n",
    "    if tie_precision is not None:\n",
    "        data = np.round(data, tie_precision)\n",
    "\n",
    "    partitions = np.concatenate(\n",
    "        [\n",
    "            [np.concatenate(data[j:j+dy*tauy:tauy,i:i+dx*taux:taux]) for i in range(nx-(dx-1)*taux)] \n",
    "            for j in range(ny-(dy-1)*tauy)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    symbols = np.apply_along_axis(np.argsort, 1, partitions)\n",
    "    symbols, symbols_count = np.unique(symbols, return_counts=True, axis=0)\n",
    "\n",
    "    probabilities = symbols_count/len(partitions)\n",
    "\n",
    "    if return_missing==False:\n",
    "        return symbols, probabilities\n",
    "    \n",
    "    else:\n",
    "        all_symbols   = list(map(list,list(itertools.permutations(np.arange(dx*dy)))))\n",
    "        miss_symbols  = setdiff(all_symbols, symbols)\n",
    "        symbols       = np.concatenate((symbols, miss_symbols))\n",
    "        probabilities = np.concatenate((probabilities, np.zeros(miss_symbols.__len__())))\n",
    "        \n",
    "        return symbols, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a48edd2-eec1-4914-b6e7-9c179fe9fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_entropy(data, dx=3, dy=1, taux=1, tauy=1, base=2, normalized=True, probs=False, tie_precision=None):\n",
    "    '''\n",
    "    Returns Permutation Entropy\n",
    "    Attributes:\n",
    "    data : array\n",
    "           Array object in the format :math:`[x_{1}, x_{2}, x_{3}, \\\\ldots ,x_{n}]`\n",
    "           or  :math:`[[x_{11}, x_{12}, x_{13}, \\\\ldots, x_{1m}],\n",
    "           \\\\ldots, [x_{n1}, x_{n2}, x_{n3}, \\\\ldots, x_{nm}]]`\n",
    "           or an ordinal probability distribution (such as the ones returned by :func:`ordpy.ordinal_distribution`).\n",
    "    dx :   int\n",
    "           Embedding dimension (horizontal axis) (default: 3).\n",
    "    dy :   int\n",
    "           Embedding dimension (vertical axis); it must be 1 for time series (default: 1).\n",
    "    taux : int\n",
    "           Embedding delay (horizontal axis) (default: 1).\n",
    "    tauy : int\n",
    "           Embedding delay (vertical axis) (default: 1).\n",
    "    base : str, int\n",
    "           Logarithm base in Shannon's entropy. Either 'e' or 2 (default: 2).\n",
    "    normalized: boolean\n",
    "                If `True`, permutation entropy is normalized by its maximum value \n",
    "                (default: `True`). If `False`, it is not.\n",
    "    probs : boolean\n",
    "            If `True`, assumes **data** is an ordinal probability distribution. If \n",
    "            `False`, **data** is expected to be a one- or two-dimensional \n",
    "            array (default: `False`). \n",
    "    tie_precision : int\n",
    "                    If not `None`, **data** is rounded with `tie_precision`\n",
    "                    number of decimals (default: `None`).\n",
    "    '''\n",
    "    if not probs:\n",
    "        _, probabilities = ordinal_distribution(data, dx, dy, taux, tauy, return_missing=False, tie_precision=tie_precision)\n",
    "    else:\n",
    "        probabilities = np.asarray(data)\n",
    "        probabilities = probabilities[probabilities>0]\n",
    "\n",
    "    if normalized==True and base in [2, '2']:        \n",
    "        smax = np.log2(float(np.math.factorial(dx*dy)))\n",
    "        s    = -np.sum(probabilities*np.log2(probabilities))\n",
    "        return s/smax\n",
    "         \n",
    "    elif normalized==True and base=='e':        \n",
    "        smax = np.log(float(np.math.factorial(dx*dy)))\n",
    "        s    = -np.sum(probabilities*np.log(probabilities))\n",
    "        return s/smax\n",
    "    \n",
    "    elif normalized==False and base in [2, '2']:\n",
    "        return -np.sum(probabilities*np.log2(probabilities))\n",
    "    else:\n",
    "        return -np.sum(probabilities*np.log(probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d66020-889e-4ac7-8c4a-2607c186f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stastical_features(dataframe):\n",
    "    '''\n",
    "    Returns a  list of statistical features such as min,max,range,mean,auto-correlation,permutation entropy for each of the features\n",
    "    Attributes:\n",
    "    dataframe - DataFrame consisting of VV,VH and VV/VH for a time period\n",
    "    '''\n",
    "    features_list = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        min_vv = min(row[0])\n",
    "        max_vv = max(row[0])\n",
    "        range_vv = max_vv - min_vv\n",
    "        mean_vv = np.mean(row[0])\n",
    "        correlation_vv = sm.tsa.acf(row[0])[1]\n",
    "        permutation_entropy_vv = permutation_entropy(row[0], dx=6,base=2, normalized=True) \n",
    "    \n",
    "        min_vh = min(row[1])\n",
    "        max_vh = max(row[1])\n",
    "        range_vh = max_vh - min_vh\n",
    "        mean_vh = np.mean(row[1])\n",
    "        correlation_vh = sm.tsa.acf(row[1])[1]\n",
    "        permutation_entropy_vh = permutation_entropy(row[1], dx=6, base=2, normalized=True)\n",
    "    \n",
    "        min_vv_by_vh = min(row[2])\n",
    "        max_vv_by_vh = max(row[2])\n",
    "        range_vv_by_vh = max_vv_by_vh - min_vv_by_vh\n",
    "        mean_vv_by_vh = np.mean(row[2])\n",
    "        correlation_vv_by_vh = sm.tsa.acf(row[2])[1]\n",
    "        permutation_entropy_vv_by_vh = permutation_entropy(row[2], dx=6, base=2, normalized=True)\n",
    "    \n",
    "        features_list.append([min_vv, max_vv, range_vv, mean_vv, correlation_vv, permutation_entropy_vv,\n",
    "                          min_vh, max_vh, range_vh,  mean_vh, correlation_vh, permutation_entropy_vh,\n",
    "                          min_vv_by_vh,  max_vv_by_vh, range_vv_by_vh, mean_vv_by_vh, correlation_vv_by_vh, permutation_entropy_vv_by_vh])\n",
    "    return features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec58f4f1-31cb-4202-9c11-9e93b1f8ca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Statistical Features for VV,VH and VV/VH and creating a dataframe\n",
    "features = generate_stastical_features(vh_vv_data)\n",
    "features_data = pd.DataFrame(features ,columns = ['min_vv', 'max_vv', 'range_vv', 'mean_vv', 'correlation_vv', 'permutation_entropy_vv',\n",
    "                          'min_vh', 'max_vh', 'range_vh', 'mean_vh', 'correlation_vh', 'permutation_entropy_vh',\n",
    "                          'min_vv_by_vh',  'max_vv_by_vh', 'range_vv_by_vh', 'mean_vv_by_vh', 'correlation_vv_by_vh', 'permutation_entropy_vv_by_vh'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c0310d-4205-4756-9641-e5bd64e248a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0955f27e-d1e8-4896-bbcd-444ba25fb242",
   "metadata": {},
   "source": [
    "## Joining the predictor variables and response variables\n",
    "Now that we have extracted our predictor variables, we need to join them onto the response variable . We use the function <i><b>combine_two_datasets</b></i> to combine the predictor variables and response variables. The <i><b>concat</b></i> function from pandas comes in handy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba0448-95b9-4960-a364-6cc3253e2750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_two_datasets(dataset1,dataset2):\n",
    "    '''\n",
    "    Returns a  vertically concatenated dataset.\n",
    "    Attributes:\n",
    "    dataset1 - Dataset 1 to be combined \n",
    "    dataset2 - Dataset 2 to be combined\n",
    "    '''\n",
    "    data = pd.concat([dataset1,dataset2], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deacc23-ae01-4b0e-8efe-622281f09fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_data = combine_two_datasets(crop_yield_data,features_data)\n",
    "crop_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff4f05e-fa7a-405b-af17-80ead4e47d2f",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f77feff-598b-4e8e-a74d-5250c0d02be7",
   "metadata": {},
   "source": [
    "<p align=\"justify\"> Now let us select the columns required for our model building exercise. Here we consider only the statistical features generated using the band values for training the model. Here we are not including latitude and longitude as predictor variables since they have no effect on the rice yield.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f61f83-f82c-4866-8665-c09c29e2f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_data = crop_data[['min_vv', 'max_vv', 'range_vv', 'mean_vv', 'correlation_vv', 'permutation_entropy_vv',\n",
    "                          'min_vh', 'max_vh', 'range_vh', 'mean_vh', 'correlation_vh', 'permutation_entropy_vh',\n",
    "                          'min_vv_by_vh',  'max_vv_by_vh', 'range_vv_by_vh', 'mean_vv_by_vh', 'correlation_vv_by_vh', 'permutation_entropy_vv_by_vh','Rice Yield (kg/ha)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb8df95-5a1b-4bd2-964e-69061e0f22d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0a9da7-f4ab-4e03-85e1-0346a50079f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd3f46b-347e-4665-8eb7-8d028046853c",
   "metadata": {},
   "source": [
    "### Train and Test Split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90847564-e4af-4aa3-ba14-e4cd1575c764",
   "metadata": {},
   "source": [
    "<p align=\"justify\">We will now split the data into 80% training data and 20% test data. Scikit-learn alias “sklearn” is a robust library for machine learning in Python. The scikit-learn library has a <i><b>model_selection</b></i> module in which there is a splitting function <i><b>train_test_split</b></i>. You can use the same.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cf613b-1ef9-4723-9259-8b735ea4eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = crop_data.drop(columns=['Rice Yield (kg/ha)']).values\n",
    "y = crop_data ['Rice Yield (kg/ha)'].values\n",
    "# Choose any random state\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d57452-ee72-4ac2-b033-296f455cf6e7",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14919aa2-2415-4b01-97d2-0646b4168fc2",
   "metadata": {},
   "source": [
    "<p justify =\"align\">Now that we have the data in a format appropriate for machine learning, we can begin training a model. In this demonstration notebook, we have used a Extra Tree Regressor  model from the scikit-learn library. This library offers a wide range of other models, each with the capacity for extensive parameter tuning and customization capabilities.</p>\n",
    "\n",
    "<p justify =\"align\">Scikit-learn models require separation of predictor variables and the response variable. You have to store the predictor variables in array X and the response variable in the array Y. You must make sure not to include the response variable in array X.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01382fe9-645a-4b63-97e0-d227b314e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
    "                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "                    max_samples=None, min_impurity_decrease=0.0, min_samples_leaf=1,\n",
    "                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                    n_estimators=100, n_jobs=-1, oob_score=False,\n",
    "                    random_state=123, verbose=0, warm_start=False)\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da06015-a2ed-4601-9783-56d98885477c",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c0e75-19ed-4eac-96b6-a7a0d42715cf",
   "metadata": {},
   "source": [
    "Now that we have trained our model , all that is left is to evaluate it. For evaluation we will generate the R2 Score. Scikit-learn provides many other metrics that can be used for evaluation. You can even write a code on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4c88ba-1f28-44a3-9af5-c1ec6ef5452d",
   "metadata": {},
   "source": [
    "### In-Sample Evaluation\n",
    "<p align=\"Jutisfy\"> We will be generating a R2 Score for the training data. It must be stressed that this is in-sample performance testing , which is the performance testing on the training dataset. These metrics are NOT truly indicative of the model's performance. You should wait to test the model performance on the test data before you feel confident about your model.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf847222-feff-474b-91fd-f2710ed69300",
   "metadata": {},
   "source": [
    "In this section, we make predictions on the training set and store them in the <b><i>insample_ predictions</i></b> variable. R2 Score is generated to gauge the robustness of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d8fc3d-5e2c-4177-b8f5-fa6f45a8bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "insample_predictions = regressor.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa951a0-861f-4fa6-94c5-05118c6712f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Insample R2 Score: {0:.2f}\".format(r2_score(y_train,insample_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccaf78a-b53b-47bd-8427-d974e1a15553",
   "metadata": {},
   "source": [
    "### Out-Sample Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7195a680-c271-4fb7-b1bb-e4cf5fc3b5e6",
   "metadata": {},
   "source": [
    "When evaluating a machine learning model, it is essential to correctly and fairly evaluate the model's ability to generalize. This is because models have a tendency to overfit/underfit the dataset they are trained on. To estimate the out-of-sample performance, we will predict on the test data now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e7d43-1eb9-4588-93ba-9c06293c9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "outsample_predictions = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc26e10-471f-46d1-af38-d295fc375e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Outsample R2 Score: {0:.2f}\".format(r2_score(y_test,outsample_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ccd250-887b-4539-9b22-800009d68994",
   "metadata": {},
   "source": [
    "From the above, we can clearly see that the model is overfitting and is able to achieve an <strong>R2 score</strong> of <b>0.25</b>. This is not a very good model, so your goal is to improve this model and the R2 Score to its maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6dec09-63a2-4170-9e93-97379f1f9039",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ccb559-4e51-430f-ac5b-4ee00b1e6068",
   "metadata": {},
   "source": [
    "Once you are happy with your model, you can make a submission. To make a submission, you will need to use your model to make the yield predictions of rice crop for a set of test coordinates we have provided in the <a href=\"https://challenge.ey.com/api/v1/storage/admin-files/8515054086281302-63ca8f827b1fe300146c7e21-challenge_2_submission_template.csv\"><b>\"challenge_2_submission_template.csv\"</b></a> file and upload the file onto the challenge platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa43a8e-d319-468d-a657-f8f1a4cb8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = pd.read_csv('challenge_2_submission_template.csv')\n",
    "test_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb45d5-1a0d-4786-9057-e9e53ba68551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Sentinel-1-RTC Data\n",
    "assests = ['vh','vv']\n",
    "submission_band_values=test_file.progress_apply(lambda x: get_sentinel_data(x['Longitude'], x['Latitude'],x['Season(SA = Summer Autumn, WS = Winter Spring)'],assests), axis=1)\n",
    "submission_vh = [x[0] for x in submission_band_values]\n",
    "submission_vv = [x[1] for x in submission_band_values]\n",
    "submission_vv_by_vh = [x[2] for x in submission_band_values]\n",
    "submission_vh_vv_data = pd.DataFrame(list(zip(submission_vh,submission_vv,submission_vv_by_vh)),columns = [\"vv_list\",\"vh_list\",\"vv/vh_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915d9e6a-0f6f-466f-b0ad-5e278d7197b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Statistical Features for VV,VH and VV/VH and creating a dataframe\n",
    "features = generate_stastical_features(submission_vh_vv_data)\n",
    "submission_features_data = pd.DataFrame(features ,columns = ['min_vv', 'max_vv', 'range_vv', 'mean_vv', 'correlation_vv', 'permutation_entropy_vv',\n",
    "                          'min_vh', 'max_vh', 'range_vh', 'mean_vh', 'correlation_vh', 'permutation_entropy_vh',\n",
    "                          'min_vv_by_vh',  'max_vv_by_vh', 'range_vv_by_vh', 'mean_vv_by_vh', 'correlation_vv_by_vh', 'permutation_entropy_vv_by_vh'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb000f-1d38-46a3-a09b-0f3fd33ec81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making predictions\n",
    "final_predictions = regressor.predict(submission_features_data.values)\n",
    "final_prediction_series = pd.Series(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede57a52-7621-453b-ba72-d1b5c4c97d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining the results into dataframe\n",
    "test_file['Predicted Rice Yield (kg/ha)']=list(final_prediction_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a2239-1fb3-41e5-9ae9-4612d792eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dumping the predictions into a csv file.\n",
    "test_file.to_csv(\"challenge_2_submission_rice_crop_yield_prediction.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d73fe-a693-4b2d-8db8-7d6b0515596e",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6237ee9-3077-47b7-ba7c-d1ed6fff934d",
   "metadata": {},
   "source": [
    "Now that you have learned a basic approach to model training, it’s time to try your own approach! Feel free to modify any of the functions presented in this notebook. We look forward to seeing your version of the model and the results. Best of luck with the challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crop-forecasting",
   "language": "python",
   "name": "crop-forecasting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
